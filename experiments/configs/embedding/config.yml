embedding:

  preprocess:
    n_bins: 51
    pre_norm: false
    include_zero_gene: false
    input_style: binned
    output_style: binned
    mask_ratio: 0.0
    input_emb_style: "continuous"  # "category" or "continuous" or "scaling"
    cell_emb_style: "cls"  # "avg-pool" or "w-pool" or "cls"
    pad_token: "<pad>"
    pad_value: -2
    special_tokens:
      - <pad>
      - <cls>
      - <eoc>
    mask_value: "auto"  # for masked values, now it should always be auto
    max_seq_len: 3001
    per_seq_batch_sample: false

  dataset:
    raw_data_key: "X"
    # data_is_raw: false unnecessary
    filter_gene_by_counts: false
    filter_cell_by_counts: false
    normalize_total: 1e4
    result_normed_key: "X_normed"
    log1p: false
    result_log1p_key: "X_log1p"
    subset_hvg: false  # 5. whether to subset the raw data to highly variable genes
    hvg_flavor: "cell_ranger"
    result_binned_key: "X_binned"


  train:
    dab_weight: 0.0
    lr: 0.0001
    batch_size: 32
    eval_batch_size: 32
    epochs: 20
    schedule_ratio: 0.9  # ratio of epochs for learning rate schedule
    schedule_interval: 1  # interval of learning rate schedule
    amp: true  # Automatic Mixed Precision
    save_eval_interval: 5
    MLM: false  # whether to use masked language modeling, currently it is always on.
    CLS: true  # celltype classification objective
    ADV: false  # Adversarial training for batch correction
#    ADV:
#      E_delay_epochs: 0  # delay adversarial training on encoder for a few epochs
#      D_delay_epochs: 0
#      lr: 0.001  # learning rate for adversarial training
    CCE: false  # Contrastive cell embedding objective
    MVC: false  # Masked value prediction for cell embedding
    ecs_thres: 0.0 # Elastic cell similarity objective, 0.0 to 1.0, 0.0 to disable
    DAB: false  # Domain adaptation by reverse backpropagation, set to 2 for separate optimizer
    INPUT_BATCH_LABELS: false  # TODO: have these help MLM and MVC, while not to classifier
    mvc_decoder_style: "inner product"
    freeze: false  # whether to freeze the transformer layers
    do_sample_in_train: false
    DSBN: false  # Domain-specific batch normalization

  model:
    embsize: 128  # embedding dimension
    nhead: 4  # number of heads in nn.MultiheadAttention
    d_hid: 128  # dimension of the feedforward network model in nn.TransformerEncoder
    nlayers: 4  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder
    nlayers_cls: 3  # number of layers specifically for classification if applicable
    n_cls: 1  # number of classes for classification, dynamically set if CLS is true
    dropout: 0.2  # dropout probability
    do_mvc: true  # Masked value prediction for cell embedding
    do_dab: false  # Domain adaptation by reverse backpropagation
    use_batch_labels: false  # whether to use batch labels for training
    domain_spec_batchnorm: false  # Domain-specific batch normalization
    explicit_zero_prob: false  # explicitly manage zero probability, tied to MLM
    use_fast_transformer: false  # whether to use a fast transformer implementation
    fast_transformer_backend: "flash"  # backend for fast transformer
    pre_norm: false  # whether to use pre-normalization in transformer layers

    # Extra
    input_emb_style: "continuous"  # style of embedding for input
    n_input_bins: TBA  # number of input bins if applicable
    cell_emb_style: "cls"  # style of embedding for cell representation
    mvc_decoder_style: "inner product"  # style of decoder for MVC
    ecs_threshold: 0.0  # Elastic cell similarity threshold


  log:
    log_interval: 100  # iterations
    save_eval_interval: 5  # epochs
    do_eval_scib_metrics: true
    retain_best_model: true  # whether to retain the best model during training